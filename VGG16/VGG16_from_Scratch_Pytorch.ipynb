{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","widgets":{"application/vnd.jupyter.widget-state+json":{"02d2b8b358914b68a3c13e651f985f6e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_325164b7d27542929d7061cdddf42921","IPY_MODEL_e8c3a05dd4de4a43899a08296462f45e","IPY_MODEL_68bcb8a4d22a4b9fa08ca2d18f78338f"],"layout":"IPY_MODEL_e80f30b26db0472a89037d6411df198f"}},"325164b7d27542929d7061cdddf42921":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e1028bb6d7dc4b3d8451b923397f92c9","placeholder":"​","style":"IPY_MODEL_b121b2cfc1fc4a8dbadf0f843e286b1e","value":"100%"}},"e8c3a05dd4de4a43899a08296462f45e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d889c346fcf1406baf39797149369836","max":170498071,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d95bb84da7ce43e383f8506e7ed841f5","value":170498071}},"68bcb8a4d22a4b9fa08ca2d18f78338f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_47c595bf7bbe4865a2dd55593ab63952","placeholder":"​","style":"IPY_MODEL_23bde38bdcee4723b0982dac0ca81634","value":" 170498071/170498071 [00:01&lt;00:00, 100084563.27it/s]"}},"e80f30b26db0472a89037d6411df198f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e1028bb6d7dc4b3d8451b923397f92c9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b121b2cfc1fc4a8dbadf0f843e286b1e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d889c346fcf1406baf39797149369836":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d95bb84da7ce43e383f8506e7ed841f5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"47c595bf7bbe4865a2dd55593ab63952":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"23bde38bdcee4723b0982dac0ca81634":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ckPCjGRJzFc6","outputId":"6b01088e-603c-4921-e3d0-91442a6ded29","executionInfo":{"status":"ok","timestamp":1676451212169,"user_tz":-210,"elapsed":13,"user":{"displayName":"yasin","userId":"00554470259111997069"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{},"execution_count":22}],"source":["import numpy as np\n","from datetime import datetime \n","import matplotlib.pyplot as plt\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torchvision import datasets\n","from torchvision import transforms\n","from torch.utils.data import DataLoader\n","\n","# Try to get torchinfo, install it if it doesn't work\n","try:\n","    from torchinfo import summary\n","except:\n","    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n","    !pip install -q torchinfo\n","    from torchinfo import summary\n","\n","# Device configuration\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","device"]},{"cell_type":"markdown","source":["# VGG16"],"metadata":{"id":"SN1Ne3IWxQ0T"}},{"cell_type":"markdown","source":["VGG is a deep convolutional neural network that was proposed by Karen Simonyan and Andrew Zisserman [1]. VGG is an acronym for their group name, Visual Geometry Group, from the Oxford University. This model secured 2nd place in the ILSVRC-2014 competition where 92.7% classification performance was achieved. The VGG model investigates the depth of layers with a very small convolutional filter size (3 × 3) to deal with large-scale images. The authors released a series of VGG models with different layer lengths, from 11 to 19, which is presented in the following table.\n","\n","* All configurations of VGG have block structures.\n","* Each VGG block consists of a sequence of convolutional layers which are followed by a max-pooling layer. The same kernel size (3 × 3) is applied over all convolutional layers. Besides, the authors used a padding size of 1 to keep the size of the output after each convolutional layer. A max-pooling of size 2 × 2 with strides of 2 is also applied to halve the resolution after each block\n","*Each VGG model has two fully connected hidden layers and one fully connected output layer."],"metadata":{"id":"_Jm2WvOoxJvz"}},{"cell_type":"markdown","source":["<img src='https://miro.medium.com/v2/resize:fit:1100/format:webp/1*NNifzsJ7tD2kAfBXt3AzEg.png' width=700 length=1300 />"],"metadata":{"id":"AJ2_50bcwbbu"}},{"cell_type":"markdown","source":["<img src='https://miro.medium.com/v2/resize:fit:1100/format:webp/1*PEvSaceAL8nNLTco_SYieg.png' width=700 length=1000 />"],"metadata":{"id":"9wH2F10fwvgb"}},{"cell_type":"code","source":["BATCH_SIZE = 32\n","\n","# define transforms\n","# transforms.ToTensor() automatically scales the images to [0,1] range\n","transforms = transforms.Compose([transforms.Resize((227, 227)), transforms.ToTensor()])\n","\n","# download and create datasets\n","train_dataset = datasets.CIFAR10(root='CIFAR10_data', train=True, transform=transforms, download=True)\n","valid_dataset = datasets.CIFAR10(root='CIFAR10_data', train=False, transform=transforms, download=True)\n","\n","# define the data loaders\n","train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","valid_loader = DataLoader(dataset=valid_dataset, batch_size=BATCH_SIZE, shuffle=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":101,"referenced_widgets":["02d2b8b358914b68a3c13e651f985f6e","325164b7d27542929d7061cdddf42921","e8c3a05dd4de4a43899a08296462f45e","68bcb8a4d22a4b9fa08ca2d18f78338f","e80f30b26db0472a89037d6411df198f","e1028bb6d7dc4b3d8451b923397f92c9","b121b2cfc1fc4a8dbadf0f843e286b1e","d889c346fcf1406baf39797149369836","d95bb84da7ce43e383f8506e7ed841f5","47c595bf7bbe4865a2dd55593ab63952","23bde38bdcee4723b0982dac0ca81634"]},"id":"3YPb3Tn8thVi","outputId":"43e67e38-e91d-4d1d-91ce-b8ec3d506e77","executionInfo":{"status":"ok","timestamp":1676453200509,"user_tz":-210,"elapsed":5312,"user":{"displayName":"yasin","userId":"00554470259111997069"}}},"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to CIFAR10_data/cifar-10-python.tar.gz\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/170498071 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02d2b8b358914b68a3c13e651f985f6e"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Extracting CIFAR10_data/cifar-10-python.tar.gz to CIFAR10_data\n","Files already downloaded and verified\n"]}]},{"cell_type":"code","source":["from torch.nn.modules.linear import Linear\n","from torch.nn.modules.container import Sequential\n","class VGG16(nn.Module):\n","    def __init__(self, num_classes=10):\n","        super(VGG16, self).__init__()\n","\n","        self.layer1 = nn.Sequential(\n","            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU())\n","        self.layer2 = nn.Sequential(\n","            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2, stride=2))\n","        self.layer3 = nn.Sequential(\n","            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(128),\n","            nn.ReLU())\n","        self.layer4 = nn.Sequential(\n","            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2, stride=2))\n","        self.layer5 = nn.Sequential(\n","            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(256),\n","            nn.ReLU())\n","        self.layer6 = nn.Sequential(\n","            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(256),\n","            nn.ReLU())\n","        self.layer7 = nn.Sequential(\n","            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2, stride=2))\n","        self.layer8 = nn.Sequential(\n","            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(512),\n","            nn.ReLU())\n","        self.layer9 = nn.Sequential(\n","            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(512),\n","            nn.ReLU())\n","        self.layer10 = nn.Sequential(\n","            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(512),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2, stride=2))\n","        self.layer11 = nn.Sequential(\n","            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(512),\n","            nn.ReLU())\n","        self.layer12 = nn.Sequential(\n","            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(512),\n","            nn.ReLU())\n","        self.layer13 = nn.Sequential(\n","            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(512),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size = 2, stride = 2))\n","        self.fc1 = nn.Sequential(\n","            nn.Dropout(0.5),\n","            nn.Linear(7*7*512, 4096),\n","            nn.ReLU())\n","        self.fc2 = nn.Sequential(\n","            nn.Dropout(0.5),\n","            nn.Linear(4096, 4096),\n","            nn.ReLU())\n","        self.fc3 = nn.Sequential(\n","            nn.Dropout(0.5),\n","            nn.Linear(4096, num_classes))\n","        \n","    def forward(self,x):\n","      out = self.layer1(x)\n","      out = self.layer2(out)\n","      out = self.layer3(out)\n","      out = self.layer4(out)\n","      out = self.layer5(out)\n","      out = self.layer6(out)\n","      out = self.layer7(out)\n","      out = self.layer8(out)\n","      out = self.layer9(out)\n","      out = self.layer10(out)\n","      out = self.layer11(out)\n","      out = self.layer12(out)\n","      out = self.layer13(out)\n","      out = out.reshape(out.size(0),-1)\n","      out = self.fc1(out)\n","      out = self.fc2(out)\n","      out = self.fc3(out)\n","\n","      return out"],"metadata":{"id":"_YGr5bx-zPcM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_accuracy(model, data_loader, device):\n","    '''\n","    Function for computing the accuracy of the predictions over the entire data_loader\n","    '''\n","\n","    correct_pred = 0\n","    n = 0\n","\n","\n","    with torch.no_grad():\n","        model.eval()\n","        for X,y_true in data_loader:\n","\n","          X = X.to(device)\n","          y_true = y_true.to(device)\n","\n","          y_hat = model(X)\n","          y_prob = F.softmax(y_hat, dim=1)\n","          _, predicted_labels = torch.max(y_prob,1)\n","\n","          n += y_true.size(0)\n","          correct_pred += (predicted_labels == y_true).sum()\n","\n","    return correct_pred.float() / n\n","\n","def plot_losses(train_losses, valid_losses):\n","    '''\n","    Function for plotting training and validation losses\n","    '''\n","\n","    # temporarily change the style of the plots to seaborn \n","    plt.style.use('seaborn')\n","\n","    train_losses = np.array(train_losses)\n","    valid_losses = np.array(valid_losses)\n","\n","    fig, ax = plt.subplots(figsize = (8, 4.5))\n","    ax.plot(train_losses, color='blue', label= 'Training loss')\n","    ax.plot(valid_losses, color='red', label= 'Validation loss')\n","    ax.set(title='Loss over epochs',\n","           xlabel='Epoch',\n","           ylabel='Loss')\n","    plt.legend()\n","    plt.show()\n","\n","    # change the plot style to default\n","    plt.style.use('default')\n","\n","def train(train_loader, model,criterion, optimizer, device):\n","    '''\n","    Function for the training step of the training loop\n","    '''\n","    \n","    model.train()    \n","    running_loss = 0\n","\n","    for X, y_true in train_loader:\n","\n","        optimizer.zero_grad()           # Sets the gradients of all optimized torch.Tensor s to zero.\n","\n","        X = X.to(device)\n","        y_true = y_true.to(device)\n","\n","        # forward paas\n","        y_hat = model(X)\n","        loss = criterion(y_hat, y_true)\n","        running_loss += loss.item()*X.size(0)        # X.size() ----> Returns the size of the self tensor\n","\n","        # Backward pass\n","        loss.backward()\n","        optimizer.step()\n","\n","    epoch_loss = running_loss / len(train_loader.dataset)\n","      \n","    return model, optimizer, epoch_loss\n","\n","def validate(valid_loader, model, criterion, device):\n","    '''\n","    Function for the validation step of the training loop\n","    '''\n","   \n","    model.eval()\n","    running_loss = 0\n","    \n","    for X, y_true in valid_loader:\n","    \n","        X = X.to(device)\n","        y_true = y_true.to(device)\n","\n","        # Forward pass and record loss\n","        y_hat = model(X) \n","        loss = criterion(y_hat, y_true) \n","        running_loss += loss.item() * X.size(0)\n","\n","    epoch_loss = running_loss / len(valid_loader.dataset)\n","        \n","    return model, epoch_loss\n","\n","def training_loop(model, criterion, optimizer, lr_scheduler, train_loader, valid_loader,epochs, device, print_every=1):\n","    '''\n","    Function defining the entire training loop\n","    '''\n","\n","    # set objects for storing metrics\n","    best_loss = 1e10\n","    train_losses = []\n","    valid_losses = []\n","\n","\n","    # Train model\n","    for epoch in range(0,epochs):\n","\n","        # training\n","        model, optimizer, train_loss = train(train_loader, model,criterion, optimizer, device)\n","        train_losses.append(train_loss)\n","\n","        # validation\n","        with torch.no_grad():\n","            model, valid_loss = validate(valid_loader, model, criterion, device)\n","            valid_losses.append(valid_loss)\n","\n","            lr_scheduler.step(valid_loss)\n","\n","        if epoch % print_every == (print_every -1):\n","\n","            train_acc = get_accuracy(model, train_loader, device=device)\n","            valid_acc = get_accuracy(model, valid_loader, device=device)\n","\n","            print(f'{datetime.now().time().replace(microsecond=0)} --- '\n","                  f'Epoch: {epoch}\\t'\n","                  f'Train loss: {train_loss:.4f}\\t'\n","                  f'Valid loss: {valid_loss:.4f}\\t'\n","                  f'Train accuracy: {100 * train_acc:.2f}\\t'\n","                  f'Valid accuracy: {100 * valid_acc:.2f}')\n","\n","    plot_losses(train_losses, valid_losses)\n","\n","    return model, optimizer, (train_losses, valid_losses)"],"metadata":{"id":"NDdCFVrN3XKo","executionInfo":{"status":"ok","timestamp":1676452420161,"user_tz":-210,"elapsed":468,"user":{"displayName":"yasin","userId":"00554470259111997069"}}},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":["# Pretrained VGG16"],"metadata":{"id":"my2BlQ9r5viK"}},{"cell_type":"code","source":["import torchvision.models as models\n","model = models.vgg16(pretrained=True)\n","\n","# Set the manual seeds\n","torch.manual_seed(42)\n","torch.cuda.manual_seed(42)\n","\n","NUM_CLASSES = 10\n","\n","# Here, we need to freeze all the network except the final layer. We need to set requires_grad = False \n","# to freeze the parameters so that the gradients are not computed in backward().\n","for param in model.parameters():\n","  param.requires_grad = False\n","\n","# Observe that only parameters of final layer are being optimized as\n","# opposed to before.\n","# Parameters of newly constructed modules have requires_grad=True by default\n","# You can see layers of your pretraind model by just write your model name in code cell and run it\n","num_ftrs = model.classifier[6].in_features\n","model.classifier[6] = nn.Linear(num_ftrs, NUM_CLASSES)"],"metadata":{"id":"60cWmmnqEPto","executionInfo":{"status":"ok","timestamp":1676453208053,"user_tz":-210,"elapsed":2037,"user":{"displayName":"yasin","userId":"00554470259111997069"}}},"execution_count":41,"outputs":[]},{"cell_type":"code","source":["N_EPOCHS = 20\n","LEARNING_RATE = 0.01\n","WEIGHT_DECAY = 0.0005\n","MOMENTUM = 0.9\n","\n","\n","model = model.to(device)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n","# Decay LR by a factor of 0.1 when a metric has stopped improving.\n","lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1)\n","\n","# Print a summary using torchinfo (uncomment for actual output)\n","summary(model=model, \n","        input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\"\n","        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n","        col_width=20,\n","        row_settings=[\"var_names\"]\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2FxFcrheyykW","outputId":"d5e5e63b-c50e-40fa-9e5e-2a1ac1cdb7b0","executionInfo":{"status":"ok","timestamp":1676453208358,"user_tz":-210,"elapsed":5,"user":{"displayName":"yasin","userId":"00554470259111997069"}}},"execution_count":42,"outputs":[{"output_type":"execute_result","data":{"text/plain":["========================================================================================================================\n","Layer (type (var_name))                  Input Shape          Output Shape         Param #              Trainable\n","========================================================================================================================\n","VGG (VGG)                                [32, 3, 224, 224]    [32, 10]             --                   Partial\n","├─Sequential (features)                  [32, 3, 224, 224]    [32, 512, 7, 7]      --                   False\n","│    └─Conv2d (0)                        [32, 3, 224, 224]    [32, 64, 224, 224]   (1,792)              False\n","│    └─ReLU (1)                          [32, 64, 224, 224]   [32, 64, 224, 224]   --                   --\n","│    └─Conv2d (2)                        [32, 64, 224, 224]   [32, 64, 224, 224]   (36,928)             False\n","│    └─ReLU (3)                          [32, 64, 224, 224]   [32, 64, 224, 224]   --                   --\n","│    └─MaxPool2d (4)                     [32, 64, 224, 224]   [32, 64, 112, 112]   --                   --\n","│    └─Conv2d (5)                        [32, 64, 112, 112]   [32, 128, 112, 112]  (73,856)             False\n","│    └─ReLU (6)                          [32, 128, 112, 112]  [32, 128, 112, 112]  --                   --\n","│    └─Conv2d (7)                        [32, 128, 112, 112]  [32, 128, 112, 112]  (147,584)            False\n","│    └─ReLU (8)                          [32, 128, 112, 112]  [32, 128, 112, 112]  --                   --\n","│    └─MaxPool2d (9)                     [32, 128, 112, 112]  [32, 128, 56, 56]    --                   --\n","│    └─Conv2d (10)                       [32, 128, 56, 56]    [32, 256, 56, 56]    (295,168)            False\n","│    └─ReLU (11)                         [32, 256, 56, 56]    [32, 256, 56, 56]    --                   --\n","│    └─Conv2d (12)                       [32, 256, 56, 56]    [32, 256, 56, 56]    (590,080)            False\n","│    └─ReLU (13)                         [32, 256, 56, 56]    [32, 256, 56, 56]    --                   --\n","│    └─Conv2d (14)                       [32, 256, 56, 56]    [32, 256, 56, 56]    (590,080)            False\n","│    └─ReLU (15)                         [32, 256, 56, 56]    [32, 256, 56, 56]    --                   --\n","│    └─MaxPool2d (16)                    [32, 256, 56, 56]    [32, 256, 28, 28]    --                   --\n","│    └─Conv2d (17)                       [32, 256, 28, 28]    [32, 512, 28, 28]    (1,180,160)          False\n","│    └─ReLU (18)                         [32, 512, 28, 28]    [32, 512, 28, 28]    --                   --\n","│    └─Conv2d (19)                       [32, 512, 28, 28]    [32, 512, 28, 28]    (2,359,808)          False\n","│    └─ReLU (20)                         [32, 512, 28, 28]    [32, 512, 28, 28]    --                   --\n","│    └─Conv2d (21)                       [32, 512, 28, 28]    [32, 512, 28, 28]    (2,359,808)          False\n","│    └─ReLU (22)                         [32, 512, 28, 28]    [32, 512, 28, 28]    --                   --\n","│    └─MaxPool2d (23)                    [32, 512, 28, 28]    [32, 512, 14, 14]    --                   --\n","│    └─Conv2d (24)                       [32, 512, 14, 14]    [32, 512, 14, 14]    (2,359,808)          False\n","│    └─ReLU (25)                         [32, 512, 14, 14]    [32, 512, 14, 14]    --                   --\n","│    └─Conv2d (26)                       [32, 512, 14, 14]    [32, 512, 14, 14]    (2,359,808)          False\n","│    └─ReLU (27)                         [32, 512, 14, 14]    [32, 512, 14, 14]    --                   --\n","│    └─Conv2d (28)                       [32, 512, 14, 14]    [32, 512, 14, 14]    (2,359,808)          False\n","│    └─ReLU (29)                         [32, 512, 14, 14]    [32, 512, 14, 14]    --                   --\n","│    └─MaxPool2d (30)                    [32, 512, 14, 14]    [32, 512, 7, 7]      --                   --\n","├─AdaptiveAvgPool2d (avgpool)            [32, 512, 7, 7]      [32, 512, 7, 7]      --                   --\n","├─Sequential (classifier)                [32, 25088]          [32, 10]             --                   Partial\n","│    └─Linear (0)                        [32, 25088]          [32, 4096]           (102,764,544)        False\n","│    └─ReLU (1)                          [32, 4096]           [32, 4096]           --                   --\n","│    └─Dropout (2)                       [32, 4096]           [32, 4096]           --                   --\n","│    └─Linear (3)                        [32, 4096]           [32, 4096]           (16,781,312)         False\n","│    └─ReLU (4)                          [32, 4096]           [32, 4096]           --                   --\n","│    └─Dropout (5)                       [32, 4096]           [32, 4096]           --                   --\n","│    └─Linear (6)                        [32, 4096]           [32, 10]             40,970               True\n","========================================================================================================================\n","Total params: 134,301,514\n","Trainable params: 40,970\n","Non-trainable params: 134,260,544\n","Total mult-adds (G): 495.35\n","========================================================================================================================\n","Input size (MB): 19.27\n","Forward/backward pass size (MB): 3470.26\n","Params size (MB): 537.21\n","Estimated Total Size (MB): 4026.74\n","========================================================================================================================"]},"metadata":{},"execution_count":42}]},{"cell_type":"code","source":["model, optimizer, _ = training_loop(model, criterion, optimizer, lr_scheduler, train_loader, valid_loader, N_EPOCHS, device)"],"metadata":{"id":"2qnPB9lZ4gpV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Evaluating the predictions"],"metadata":{"id":"oLYNhCbUFHlO"}},{"cell_type":"code","source":["N_COLS = 10\n","N_ROWS = 5\n","\n","fig = plt.figure()\n","for index in range(1,  N_COLS* N_ROWS + 1):\n","    plt.subplot(N_ROWS, N_COLS, index)\n","    plt.axis('off')\n","    plt.imshow(valid_dataset.data[index], cmap='gray_r')\n","\n","    with torch.no_grad():\n","        model.eval()\n","        y_hat = model(valid_dataset[index][0].unsqueeze(0))\n","        probs = F.softmax(y_hat, dim=1)\n","\n","    title = f'{torch.argmax(probs)}  ({torch.max(probs * 100):.0f}%)'\n","\n","    plt.title(title, fontsize=7)\n","fig.suptitle('LeNet-5 - predictions');"],"metadata":{"id":"HsrOQskcE7qd"},"execution_count":null,"outputs":[]}]}