{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOAabdJ8aOBVUb1ddQY0bFv"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"23bd3a057752471eb7187cd72715ca39":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0628cc08706d48498f4ec8daa8217cb6","IPY_MODEL_435504478ab148b48f117c3c743cfb90","IPY_MODEL_876d51d3c625403c8bad24fc0dacb01d"],"layout":"IPY_MODEL_150e2096b37e4189bd64b661dc430c28"}},"0628cc08706d48498f4ec8daa8217cb6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6fd6746c7a554daa8182af23cad515a3","placeholder":"​","style":"IPY_MODEL_45364b6b50954c40aa87dec70c09d33f","value":"100%"}},"435504478ab148b48f117c3c743cfb90":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8dbabf7dc7fd471e9efffb6d4735c001","max":170498071,"min":0,"orientation":"horizontal","style":"IPY_MODEL_eac27f9bcc0b4a8ba2e2e8a7b780b082","value":170498071}},"876d51d3c625403c8bad24fc0dacb01d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1be57c81b2c848e9908640f2c6e33e70","placeholder":"​","style":"IPY_MODEL_d2fc864e03594ed6bb536a9c8a61296c","value":" 170498071/170498071 [00:06&lt;00:00, 38402891.48it/s]"}},"150e2096b37e4189bd64b661dc430c28":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6fd6746c7a554daa8182af23cad515a3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"45364b6b50954c40aa87dec70c09d33f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8dbabf7dc7fd471e9efffb6d4735c001":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eac27f9bcc0b4a8ba2e2e8a7b780b082":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1be57c81b2c848e9908640f2c6e33e70":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d2fc864e03594ed6bb536a9c8a61296c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u_Q6RZiXQnjI","executionInfo":{"status":"ok","timestamp":1678448656431,"user_tz":-210,"elapsed":17110,"user":{"displayName":"yasin","userId":"00554470259111997069"}},"outputId":"5311f978-b179-4389-e23b-70b7b7fba951"},"outputs":[{"output_type":"stream","name":"stdout","text":["[INFO] Couldn't find torchinfo... installing it.\n","[INFO] Couldn't find going_modular scripts... downloading them from GitHub.\n","Cloning into 'FromScratch'...\n","remote: Enumerating objects: 187, done.\u001b[K\n","remote: Counting objects: 100% (81/81), done.\u001b[K\n","remote: Compressing objects: 100% (54/54), done.\u001b[K\n","remote: Total 187 (delta 49), reused 50 (delta 27), pack-reused 106\u001b[K\n","Receiving objects: 100% (187/187), 2.07 MiB | 9.88 MiB/s, done.\n","Resolving deltas: 100% (91/91), done.\n"]}],"source":["# Continue with regular imports\n","\n","import torch.optim as optim\n","from torch.optim import lr_scheduler\n","from torch.autograd import Variable\n","import numpy as np\n","import torchvision\n","from torchvision import datasets, models, transforms\n","import torch.nn.functional as F\n","import matplotlib.pyplot as plt\n","import time\n","import os\n","import copy\n","import sklearn.svm\n","from sklearn.model_selection import train_test_split, KFold\n","from sklearn.metrics import classification_report, confusion_matrix  \n","from collections import Counter\n","import random\n","\n","import matplotlib.pyplot as plt\n","import torch\n","import torchvision\n","\n","from torch import nn\n","from torchvision import transforms\n","from torchvision.transforms import ToTensor\n","from torchvision import datasets\n","from torch.utils.data import DataLoader\n","\n","# Try to get torchinfo, install it if it doesn't work\n","try:\n","    from torchinfo import summary\n","except:\n","    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n","    !pip install -q torchinfo\n","    from torchinfo import summary\n","\n","# Try to import the Pytorch_Modules directory, download it from GitHub if it doesn't work\n","try:\n","    from Pytorch_Modules import engine, prediction, utils, alexnet\n","except:\n","    # Get the Pytorch_Modules scripts\n","    print(\"[INFO] Couldn't find going_modular scripts... downloading them from GitHub.\")\n","    !git clone https://github.com/yasinahmadpoor/FromScratch\n","    !mv FromScratch/Pytorch_Modules .\n","    !rm -rf FromScratch\n","    from Pytorch_Modules import engine, prediction, utils\n","\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\""]},{"cell_type":"code","source":["def get_train_test_loader(data_dir,\n","                           batch_size,\n","                           augment,\n","                           shuffle=True):\n","\n","    # define normalizer\n","    normalize = transforms.Normalize(\n","            mean=[0.4914, 0.4822, 0.4465],\n","            std=[0.2023, 0.1994, 0.2010])    # Normalize a tensor image with mean and standard deviation for n channels, this transform will normalize each channel of the input.\n","\n","\n","    # define transforms\n","    valid_transform = transforms.Compose([\n","                transforms.Resize((227,227)),\n","                transforms.ToTensor(),          # transforms.ToTensor() automatically scales the images to [0,1] range\n","                normalize])\n","\n","\n","    if augment:\n","            train_transform = transforms.Compose([\n","                transforms.RandomCrop(32, padding=4),\n","                transforms.RandomHorizontalFlip(),\n","                transforms.ToTensor(),\n","                normalize,\n","            ])\n","    else:\n","            train_transform = transforms.Compose([\n","                transforms.Resize((227,227)),\n","                transforms.ToTensor(),\n","                normalize,\n","            ])\n","\n","    # download and create datasets\n","    train_dataset = datasets.CIFAR10(root=data_dir, train=True, transform=train_transform, download=True)\n","    valid_dataset = datasets.CIFAR10(root=data_dir, train=False, transform=valid_transform, download=True)\n","\n","    # define the data loaders\n","    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle = shuffle)\n","    valid_loader = DataLoader(dataset=valid_dataset, batch_size=batch_size, shuffle=shuffle)\n","\n","    return (train_dataset, valid_dataset, train_loader, valid_loader)\n","\n","BATCH_SIZE = 64\n","train_data, test_data, train_dataloader, test_dataloader = get_train_test_loader(data_dir = 'CIFAR10_data',\n","                                                    batch_size = BATCH_SIZE,\n","                                                    augment = False)\n","# Let's check out what we've created\n","print(f\"Dataloaders: {train_dataloader, test_dataloader}\") \n","print(f\"Length of train dataloader: {len(train_dataloader)} batches of {BATCH_SIZE}\")\n","print(f\"Length of test dataloader: {len(test_dataloader)} batches of {BATCH_SIZE}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":153,"referenced_widgets":["23bd3a057752471eb7187cd72715ca39","0628cc08706d48498f4ec8daa8217cb6","435504478ab148b48f117c3c743cfb90","876d51d3c625403c8bad24fc0dacb01d","150e2096b37e4189bd64b661dc430c28","6fd6746c7a554daa8182af23cad515a3","45364b6b50954c40aa87dec70c09d33f","8dbabf7dc7fd471e9efffb6d4735c001","eac27f9bcc0b4a8ba2e2e8a7b780b082","1be57c81b2c848e9908640f2c6e33e70","d2fc864e03594ed6bb536a9c8a61296c"]},"id":"NeleMaWxVZVn","executionInfo":{"status":"ok","timestamp":1678448670331,"user_tz":-210,"elapsed":13913,"user":{"displayName":"yasin","userId":"00554470259111997069"}},"outputId":"1b853466-ffab-4ad8-e84b-0c77e72cc4ad"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to CIFAR10_data/cifar-10-python.tar.gz\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/170498071 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"23bd3a057752471eb7187cd72715ca39"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Extracting CIFAR10_data/cifar-10-python.tar.gz to CIFAR10_data\n","Files already downloaded and verified\n","Dataloaders: (<torch.utils.data.dataloader.DataLoader object at 0x7fead5527370>, <torch.utils.data.dataloader.DataLoader object at 0x7feb74667880>)\n","Length of train dataloader: 782 batches of 64\n","Length of test dataloader: 157 batches of 64\n"]}]},{"cell_type":"code","source":["def set_up_network(freeze_training = True, pretrained=True, clip_classifier = True):    \n","    network = torch.hub.load('pytorch/vision:v0.10.0', 'alexnet', pretrained=pretrained)\n","\n","    if clip_classifier:\n","        features = list(network.classifier.children())[:-1] # Remove last layer\n","        network.classifier = nn.Sequential(*features) # Replace the model classifier\n","\n","    if freeze_training:\n","        for param in network.parameters():\n","            param.require_grad = False\n","    return network"],"metadata":{"id":"odEYLu7OYzqO","executionInfo":{"status":"ok","timestamp":1678451026515,"user_tz":-210,"elapsed":5,"user":{"displayName":"yasin","userId":"00554470259111997069"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","source":["models = set_up_network(freeze_training = True, pretrained=True, clip_classifier = True)\n","\n","summary(model=models, \n","        input_size=(32, 3, 227, 227), # make sure this is \"input_size\", not \"input_shape\"\n","        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n","        col_width=20,\n","        row_settings=[\"var_names\"]\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2qOmgBQKaTUf","executionInfo":{"status":"ok","timestamp":1678451101256,"user_tz":-210,"elapsed":3753,"user":{"displayName":"yasin","userId":"00554470259111997069"}},"outputId":"cdf51338-badd-4608-bf46-78e0fc26748b"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stderr","text":["Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n","/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]},{"output_type":"execute_result","data":{"text/plain":["========================================================================================================================\n","Layer (type (var_name))                  Input Shape          Output Shape         Param #              Trainable\n","========================================================================================================================\n","AlexNet (AlexNet)                        [32, 3, 227, 227]    [32, 4096]           --                   True\n","├─Sequential (features)                  [32, 3, 227, 227]    [32, 256, 6, 6]      --                   True\n","│    └─Conv2d (0)                        [32, 3, 227, 227]    [32, 64, 56, 56]     23,296               True\n","│    └─ReLU (1)                          [32, 64, 56, 56]     [32, 64, 56, 56]     --                   --\n","│    └─MaxPool2d (2)                     [32, 64, 56, 56]     [32, 64, 27, 27]     --                   --\n","│    └─Conv2d (3)                        [32, 64, 27, 27]     [32, 192, 27, 27]    307,392              True\n","│    └─ReLU (4)                          [32, 192, 27, 27]    [32, 192, 27, 27]    --                   --\n","│    └─MaxPool2d (5)                     [32, 192, 27, 27]    [32, 192, 13, 13]    --                   --\n","│    └─Conv2d (6)                        [32, 192, 13, 13]    [32, 384, 13, 13]    663,936              True\n","│    └─ReLU (7)                          [32, 384, 13, 13]    [32, 384, 13, 13]    --                   --\n","│    └─Conv2d (8)                        [32, 384, 13, 13]    [32, 256, 13, 13]    884,992              True\n","│    └─ReLU (9)                          [32, 256, 13, 13]    [32, 256, 13, 13]    --                   --\n","│    └─Conv2d (10)                       [32, 256, 13, 13]    [32, 256, 13, 13]    590,080              True\n","│    └─ReLU (11)                         [32, 256, 13, 13]    [32, 256, 13, 13]    --                   --\n","│    └─MaxPool2d (12)                    [32, 256, 13, 13]    [32, 256, 6, 6]      --                   --\n","├─AdaptiveAvgPool2d (avgpool)            [32, 256, 6, 6]      [32, 256, 6, 6]      --                   --\n","├─Sequential (classifier)                [32, 9216]           [32, 4096]           --                   True\n","│    └─Dropout (0)                       [32, 9216]           [32, 9216]           --                   --\n","│    └─Linear (1)                        [32, 9216]           [32, 4096]           37,752,832           True\n","│    └─ReLU (2)                          [32, 4096]           [32, 4096]           --                   --\n","│    └─Dropout (3)                       [32, 4096]           [32, 4096]           --                   --\n","│    └─Linear (4)                        [32, 4096]           [32, 4096]           16,781,312           True\n","│    └─ReLU (5)                          [32, 4096]           [32, 4096]           --                   --\n","========================================================================================================================\n","Total params: 57,003,840\n","Trainable params: 57,003,840\n","Non-trainable params: 0\n","Total mult-adds (G): 22.82\n","========================================================================================================================\n","Input size (MB): 19.79\n","Forward/backward pass size (MB): 128.07\n","Params size (MB): 228.02\n","Estimated Total Size (MB): 375.88\n","========================================================================================================================"]},"metadata":{},"execution_count":36}]},{"cell_type":"code","source":["\n","def set_CNN():\n","    # Set the manual seeds\n","    torch.manual_seed(42)\n","    torch.cuda.manual_seed(42)\n","\n","    model = torch.hub.load('pytorch/vision:v0.10.0', 'alexnet', pretrained=True)\n","    # model.eval()\n","\n","    NUM_CLASSES = 10\n","\n","    # Freeze all base layers in the \"features\" section of the model (the feature extractor) by setting requires_grad=False\n","    for param in model.parameters():\n","      param.requires_grad = False\n","\n","    features = list(model.classifier.children())[:-1] # Remove last layer\n","    model.classifier = nn.Sequential(*features) # Replace the model classifier\n","    model.classifier[4].requires_grad = True\n","\n","    return model\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3YRBkUfVVcWm","executionInfo":{"status":"ok","timestamp":1678451727255,"user_tz":-210,"elapsed":1696,"user":{"displayName":"yasin","userId":"00554470259111997069"}},"outputId":"535bb67c-5a0a-40ab-9ef8-2235cde53acf"},"execution_count":58,"outputs":[{"output_type":"stream","name":"stderr","text":["Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n"]}]},{"cell_type":"code","source":["# Print a summary using torchinfo (uncomment for actual output)\n","model = set_CNN()\n","summary(model=model, \n","        input_size=(32, 3, 227, 227), # make sure this is \"input_size\", not \"input_shape\"\n","        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n","        col_width=20,\n","        row_settings=[\"var_names\"]\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LYza4KD6VmXu","executionInfo":{"status":"ok","timestamp":1678451727745,"user_tz":-210,"elapsed":496,"user":{"displayName":"yasin","userId":"00554470259111997069"}},"outputId":"bb94f040-4b99-408a-fd2d-a35843a7a686"},"execution_count":59,"outputs":[{"output_type":"execute_result","data":{"text/plain":["========================================================================================================================\n","Layer (type (var_name))                  Input Shape          Output Shape         Param #              Trainable\n","========================================================================================================================\n","AlexNet (AlexNet)                        [32, 3, 227, 227]    [32, 4096]           --                   False\n","├─Sequential (features)                  [32, 3, 227, 227]    [32, 256, 6, 6]      --                   False\n","│    └─Conv2d (0)                        [32, 3, 227, 227]    [32, 64, 56, 56]     (23,296)             False\n","│    └─ReLU (1)                          [32, 64, 56, 56]     [32, 64, 56, 56]     --                   --\n","│    └─MaxPool2d (2)                     [32, 64, 56, 56]     [32, 64, 27, 27]     --                   --\n","│    └─Conv2d (3)                        [32, 64, 27, 27]     [32, 192, 27, 27]    (307,392)            False\n","│    └─ReLU (4)                          [32, 192, 27, 27]    [32, 192, 27, 27]    --                   --\n","│    └─MaxPool2d (5)                     [32, 192, 27, 27]    [32, 192, 13, 13]    --                   --\n","│    └─Conv2d (6)                        [32, 192, 13, 13]    [32, 384, 13, 13]    (663,936)            False\n","│    └─ReLU (7)                          [32, 384, 13, 13]    [32, 384, 13, 13]    --                   --\n","│    └─Conv2d (8)                        [32, 384, 13, 13]    [32, 256, 13, 13]    (884,992)            False\n","│    └─ReLU (9)                          [32, 256, 13, 13]    [32, 256, 13, 13]    --                   --\n","│    └─Conv2d (10)                       [32, 256, 13, 13]    [32, 256, 13, 13]    (590,080)            False\n","│    └─ReLU (11)                         [32, 256, 13, 13]    [32, 256, 13, 13]    --                   --\n","│    └─MaxPool2d (12)                    [32, 256, 13, 13]    [32, 256, 6, 6]      --                   --\n","├─AdaptiveAvgPool2d (avgpool)            [32, 256, 6, 6]      [32, 256, 6, 6]      --                   --\n","├─Sequential (classifier)                [32, 9216]           [32, 4096]           --                   False\n","│    └─Dropout (0)                       [32, 9216]           [32, 9216]           --                   --\n","│    └─Linear (1)                        [32, 9216]           [32, 4096]           (37,752,832)         False\n","│    └─ReLU (2)                          [32, 4096]           [32, 4096]           --                   --\n","│    └─Dropout (3)                       [32, 4096]           [32, 4096]           --                   --\n","│    └─Linear (4)                        [32, 4096]           [32, 4096]           (16,781,312)         False\n","│    └─ReLU (5)                          [32, 4096]           [32, 4096]           --                   --\n","========================================================================================================================\n","Total params: 57,003,840\n","Trainable params: 0\n","Non-trainable params: 57,003,840\n","Total mult-adds (G): 22.82\n","========================================================================================================================\n","Input size (MB): 19.79\n","Forward/backward pass size (MB): 128.07\n","Params size (MB): 228.02\n","Estimated Total Size (MB): 375.88\n","========================================================================================================================"]},"metadata":{},"execution_count":59}]},{"cell_type":"code","source":["LEARNING_RATE = 0.005\n","loss_fn = nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9, weight_decay=0.005)"],"metadata":{"id":"vQyyj4oiVmUQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Start training with help from engine.py\n","NUM_EPOCHS = 5\n","result = engine.train(model=model,\n","             train_dataloader=train_dataloader,\n","             test_dataloader=test_dataloader,\n","             loss_fn=loss_fn,\n","             optimizer=optimizer,\n","             epochs=NUM_EPOCHS,\n","             device=device)"],"metadata":{"id":"bqnGauWnVpt6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Save the model with help from utils.py\n","utils.save_model(model=model,\n","                 target_dir=\"models\",\n","                 model_name=\"05_going_modular_script_mode_tinyvgg_model.pth\")"],"metadata":{"id":"aXkAqrydVuYo"},"execution_count":null,"outputs":[]}]}