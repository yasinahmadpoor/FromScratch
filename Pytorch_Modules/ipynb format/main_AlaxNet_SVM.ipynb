{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO/1IdfHR3iXxhlNpf+qkcn"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0e8a485d0f0e4f50a6bef64d5c4aca6b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_635e4150470149fe8128f95ccf05e657","IPY_MODEL_a1d770c6722249599c3c326e2f9f9c41","IPY_MODEL_2c17c2f3f6da4ce3b654380d35d16376"],"layout":"IPY_MODEL_0e984bab3b104a3590336848613087ee"}},"635e4150470149fe8128f95ccf05e657":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4eb84716b30e4f25ad37aba074d1b04d","placeholder":"​","style":"IPY_MODEL_73628381ccd942f19e843f8ecc83c646","value":"100%"}},"a1d770c6722249599c3c326e2f9f9c41":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_72326240e4224b7f92b2e1445162d6a9","max":244408911,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6897ff2615434e9783444a7402ac68ef","value":244408911}},"2c17c2f3f6da4ce3b654380d35d16376":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fff81e0ac18d49fdabf22220d69372e7","placeholder":"​","style":"IPY_MODEL_f045e31d2a5c4ecc8f7f33362295f427","value":" 233M/233M [00:02&lt;00:00, 87.9MB/s]"}},"0e984bab3b104a3590336848613087ee":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4eb84716b30e4f25ad37aba074d1b04d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"73628381ccd942f19e843f8ecc83c646":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"72326240e4224b7f92b2e1445162d6a9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6897ff2615434e9783444a7402ac68ef":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fff81e0ac18d49fdabf22220d69372e7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f045e31d2a5c4ecc8f7f33362295f427":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"u_Q6RZiXQnjI","executionInfo":{"status":"ok","timestamp":1678474394844,"user_tz":-210,"elapsed":6498,"user":{"displayName":"yasin","userId":"00554470259111997069"}}},"outputs":[],"source":["# Continue with regular imports\n","\n","import torch.optim as optim\n","from torch.optim import lr_scheduler\n","from torch.autograd import Variable\n","import numpy as np\n","import torchvision\n","from torchvision import datasets, models, transforms\n","import torch.nn.functional as F\n","import matplotlib.pyplot as plt\n","import time\n","import os\n","import copy\n","import sklearn.svm\n","from sklearn.model_selection import train_test_split, KFold\n","from sklearn.metrics import classification_report, confusion_matrix  \n","from collections import Counter\n","import random\n","\n","import matplotlib.pyplot as plt\n","import torch\n","import torchvision\n","\n","from torch import nn\n","from torchvision import transforms\n","from torchvision.transforms import ToTensor\n","from torchvision import datasets\n","from torch.utils.data import DataLoader\n","\n","# Try to get torchinfo, install it if it doesn't work\n","try:\n","    from torchinfo import summary\n","except:\n","    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n","    !pip install -q torchinfo\n","    from torchinfo import summary\n","\n","# Try to import the Pytorch_Modules directory, download it from GitHub if it doesn't work\n","try:\n","    from Pytorch_Modules import engine, prediction, utils, alexnet\n","except:\n","    # Get the Pytorch_Modules scripts\n","    print(\"[INFO] Couldn't find going_modular scripts... downloading them from GitHub.\")\n","    !git clone https://github.com/yasinahmadpoor/FromScratch\n","    !mv FromScratch/Pytorch_Modules .\n","    !rm -rf FromScratch\n","    from Pytorch_Modules import engine, prediction, utils\n","\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\""]},{"cell_type":"code","source":["def get_train_test_loader(data_dir,\n","                           batch_size,\n","                           augment,\n","                           shuffle=True):\n","\n","    # define normalizer\n","    normalize = transforms.Normalize(\n","            mean=[0.4914, 0.4822, 0.4465],\n","            std=[0.2023, 0.1994, 0.2010])    # Normalize a tensor image with mean and standard deviation for n channels, this transform will normalize each channel of the input.\n","\n","\n","    # define transforms\n","    valid_transform = transforms.Compose([\n","                transforms.Resize((227,227)),\n","                transforms.ToTensor(),          # transforms.ToTensor() automatically scales the images to [0,1] range\n","                normalize])\n","\n","\n","    if augment:\n","            train_transform = transforms.Compose([\n","                transforms.RandomCrop(32, padding=4),\n","                transforms.RandomHorizontalFlip(),\n","                transforms.ToTensor(),\n","                normalize,\n","            ])\n","    else:\n","            train_transform = transforms.Compose([\n","                transforms.Resize((227,227)),\n","                transforms.ToTensor(),\n","                normalize,\n","            ])\n","\n","    # download and create datasets\n","    train_dataset = datasets.CIFAR10(root=data_dir, train=True, transform=train_transform, download=True)\n","    valid_dataset = datasets.CIFAR10(root=data_dir, train=False, transform=valid_transform, download=True)\n","\n","    # define the data loaders\n","    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle = shuffle)\n","    valid_loader = DataLoader(dataset=valid_dataset, batch_size=batch_size, shuffle=shuffle)\n","\n","    return (train_dataset, valid_dataset, train_loader, valid_loader)\n","\n","BATCH_SIZE = 64\n","train_data, test_data, train_dataloader, test_dataloader = get_train_test_loader(data_dir = 'CIFAR10_data',\n","                                                    batch_size = BATCH_SIZE,\n","                                                    augment = False)\n","# Let's check out what we've created\n","print(f\"Dataloaders: {train_dataloader, test_dataloader}\") \n","print(f\"Length of train dataloader: {len(train_dataloader)} batches of {BATCH_SIZE}\")\n","print(f\"Length of test dataloader: {len(test_dataloader)} batches of {BATCH_SIZE}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NeleMaWxVZVn","executionInfo":{"status":"ok","timestamp":1678474397075,"user_tz":-210,"elapsed":2253,"user":{"displayName":"yasin","userId":"00554470259111997069"}},"outputId":"490f1f96-5c1f-4a1c-f506-680b7db99bb9"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Files already downloaded and verified\n","Files already downloaded and verified\n","Dataloaders: (<torch.utils.data.dataloader.DataLoader object at 0x7fbf1678cd30>, <torch.utils.data.dataloader.DataLoader object at 0x7fbf1678cf10>)\n","Length of train dataloader: 782 batches of 64\n","Length of test dataloader: 157 batches of 64\n"]}]},{"cell_type":"code","source":["next(iter(train_dataloader))[0].shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WR5M9FbU2jot","executionInfo":{"status":"ok","timestamp":1678474426971,"user_tz":-210,"elapsed":622,"user":{"displayName":"yasin","userId":"00554470259111997069"}},"outputId":"0bea6e48-c039-47e0-f857-6ed0c324eef3"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([64, 3, 227, 227])"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["def set_up_network(freeze_training = True, pretrained=True, clip_classifier = True):    \n","    network = torch.hub.load('pytorch/vision:v0.10.0', 'alexnet', pretrained=pretrained)\n","\n","    if clip_classifier:\n","        features = list(network.classifier.children())[:-1] # Remove last layer\n","        network.classifier = nn.Sequential(*features) # Replace the model classifier\n","\n","    if freeze_training:\n","        for param in network.parameters():\n","            param.require_grad = False\n","    return network"],"metadata":{"id":"odEYLu7OYzqO","executionInfo":{"status":"ok","timestamp":1678473602185,"user_tz":-210,"elapsed":8,"user":{"displayName":"yasin","userId":"00554470259111997069"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["models = set_up_network(freeze_training = True, pretrained=True, clip_classifier = True)\n","\n","summary(model=models, \n","        input_size=(32, 3, 227, 227), # make sure this is \"input_size\", not \"input_shape\"\n","        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n","        col_width=20,\n","        row_settings=[\"var_names\"]\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":815,"referenced_widgets":["0e8a485d0f0e4f50a6bef64d5c4aca6b","635e4150470149fe8128f95ccf05e657","a1d770c6722249599c3c326e2f9f9c41","2c17c2f3f6da4ce3b654380d35d16376","0e984bab3b104a3590336848613087ee","4eb84716b30e4f25ad37aba074d1b04d","73628381ccd942f19e843f8ecc83c646","72326240e4224b7f92b2e1445162d6a9","6897ff2615434e9783444a7402ac68ef","fff81e0ac18d49fdabf22220d69372e7","f045e31d2a5c4ecc8f7f33362295f427"]},"id":"2qOmgBQKaTUf","executionInfo":{"status":"ok","timestamp":1678473611113,"user_tz":-210,"elapsed":8934,"user":{"displayName":"yasin","userId":"00554470259111997069"}},"outputId":"41c703e0-6928-4fbb-e5dd-24466d2d7481"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"https://github.com/pytorch/vision/zipball/v0.10.0\" to /root/.cache/torch/hub/v0.10.0.zip\n","/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /root/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0.00/233M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e8a485d0f0e4f50a6bef64d5c4aca6b"}},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["========================================================================================================================\n","Layer (type (var_name))                  Input Shape          Output Shape         Param #              Trainable\n","========================================================================================================================\n","AlexNet (AlexNet)                        [32, 3, 227, 227]    [32, 4096]           --                   True\n","├─Sequential (features)                  [32, 3, 227, 227]    [32, 256, 6, 6]      --                   True\n","│    └─Conv2d (0)                        [32, 3, 227, 227]    [32, 64, 56, 56]     23,296               True\n","│    └─ReLU (1)                          [32, 64, 56, 56]     [32, 64, 56, 56]     --                   --\n","│    └─MaxPool2d (2)                     [32, 64, 56, 56]     [32, 64, 27, 27]     --                   --\n","│    └─Conv2d (3)                        [32, 64, 27, 27]     [32, 192, 27, 27]    307,392              True\n","│    └─ReLU (4)                          [32, 192, 27, 27]    [32, 192, 27, 27]    --                   --\n","│    └─MaxPool2d (5)                     [32, 192, 27, 27]    [32, 192, 13, 13]    --                   --\n","│    └─Conv2d (6)                        [32, 192, 13, 13]    [32, 384, 13, 13]    663,936              True\n","│    └─ReLU (7)                          [32, 384, 13, 13]    [32, 384, 13, 13]    --                   --\n","│    └─Conv2d (8)                        [32, 384, 13, 13]    [32, 256, 13, 13]    884,992              True\n","│    └─ReLU (9)                          [32, 256, 13, 13]    [32, 256, 13, 13]    --                   --\n","│    └─Conv2d (10)                       [32, 256, 13, 13]    [32, 256, 13, 13]    590,080              True\n","│    └─ReLU (11)                         [32, 256, 13, 13]    [32, 256, 13, 13]    --                   --\n","│    └─MaxPool2d (12)                    [32, 256, 13, 13]    [32, 256, 6, 6]      --                   --\n","├─AdaptiveAvgPool2d (avgpool)            [32, 256, 6, 6]      [32, 256, 6, 6]      --                   --\n","├─Sequential (classifier)                [32, 9216]           [32, 4096]           --                   True\n","│    └─Dropout (0)                       [32, 9216]           [32, 9216]           --                   --\n","│    └─Linear (1)                        [32, 9216]           [32, 4096]           37,752,832           True\n","│    └─ReLU (2)                          [32, 4096]           [32, 4096]           --                   --\n","│    └─Dropout (3)                       [32, 4096]           [32, 4096]           --                   --\n","│    └─Linear (4)                        [32, 4096]           [32, 4096]           16,781,312           True\n","│    └─ReLU (5)                          [32, 4096]           [32, 4096]           --                   --\n","========================================================================================================================\n","Total params: 57,003,840\n","Trainable params: 57,003,840\n","Non-trainable params: 0\n","Total mult-adds (G): 22.82\n","========================================================================================================================\n","Input size (MB): 19.79\n","Forward/backward pass size (MB): 128.07\n","Params size (MB): 228.02\n","Estimated Total Size (MB): 375.88\n","========================================================================================================================"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["\n","def set_AlexNet():\n","    # Set the manual seeds\n","    torch.manual_seed(42)\n","    torch.cuda.manual_seed(42)\n","\n","    model = torch.hub.load('pytorch/vision:v0.10.0', 'alexnet', pretrained=True)\n","    # model.eval()\n","\n","\n","    # Freeze all base layers in the \"features\" section of the model (the feature extractor) by setting requires_grad=False\n","    for param in model.parameters():\n","      param.requires_grad = False\n","\n","    features = list(model.classifier.children())[:-1] # Remove last layer\n","    model.classifier = nn.Sequential(*features) # Replace the model classifier\n","    model.classifier[4].requires_grad = True\n","\n","    return model\n"],"metadata":{"id":"3YRBkUfVVcWm","executionInfo":{"status":"ok","timestamp":1678474660229,"user_tz":-210,"elapsed":343,"user":{"displayName":"yasin","userId":"00554470259111997069"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# Print a summary using torchinfo (uncomment for actual output)\n","model = set_AlexNet()\n","summary(model=model, \n","        input_size=(32, 3, 227, 227), # make sure this is \"input_size\", not \"input_shape\"\n","        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n","        col_width=20,\n","        row_settings=[\"var_names\"]\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LYza4KD6VmXu","executionInfo":{"status":"ok","timestamp":1678474678065,"user_tz":-210,"elapsed":4142,"user":{"displayName":"yasin","userId":"00554470259111997069"}},"outputId":"9d3fe06b-4c77-47ac-cce1-bcac930b80c6"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stderr","text":["Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n","/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]},{"output_type":"execute_result","data":{"text/plain":["========================================================================================================================\n","Layer (type (var_name))                  Input Shape          Output Shape         Param #              Trainable\n","========================================================================================================================\n","AlexNet (AlexNet)                        [32, 3, 227, 227]    [32, 4096]           --                   False\n","├─Sequential (features)                  [32, 3, 227, 227]    [32, 256, 6, 6]      --                   False\n","│    └─Conv2d (0)                        [32, 3, 227, 227]    [32, 64, 56, 56]     (23,296)             False\n","│    └─ReLU (1)                          [32, 64, 56, 56]     [32, 64, 56, 56]     --                   --\n","│    └─MaxPool2d (2)                     [32, 64, 56, 56]     [32, 64, 27, 27]     --                   --\n","│    └─Conv2d (3)                        [32, 64, 27, 27]     [32, 192, 27, 27]    (307,392)            False\n","│    └─ReLU (4)                          [32, 192, 27, 27]    [32, 192, 27, 27]    --                   --\n","│    └─MaxPool2d (5)                     [32, 192, 27, 27]    [32, 192, 13, 13]    --                   --\n","│    └─Conv2d (6)                        [32, 192, 13, 13]    [32, 384, 13, 13]    (663,936)            False\n","│    └─ReLU (7)                          [32, 384, 13, 13]    [32, 384, 13, 13]    --                   --\n","│    └─Conv2d (8)                        [32, 384, 13, 13]    [32, 256, 13, 13]    (884,992)            False\n","│    └─ReLU (9)                          [32, 256, 13, 13]    [32, 256, 13, 13]    --                   --\n","│    └─Conv2d (10)                       [32, 256, 13, 13]    [32, 256, 13, 13]    (590,080)            False\n","│    └─ReLU (11)                         [32, 256, 13, 13]    [32, 256, 13, 13]    --                   --\n","│    └─MaxPool2d (12)                    [32, 256, 13, 13]    [32, 256, 6, 6]      --                   --\n","├─AdaptiveAvgPool2d (avgpool)            [32, 256, 6, 6]      [32, 256, 6, 6]      --                   --\n","├─Sequential (classifier)                [32, 9216]           [32, 4096]           --                   False\n","│    └─Dropout (0)                       [32, 9216]           [32, 9216]           --                   --\n","│    └─Linear (1)                        [32, 9216]           [32, 4096]           (37,752,832)         False\n","│    └─ReLU (2)                          [32, 4096]           [32, 4096]           --                   --\n","│    └─Dropout (3)                       [32, 4096]           [32, 4096]           --                   --\n","│    └─Linear (4)                        [32, 4096]           [32, 4096]           (16,781,312)         False\n","│    └─ReLU (5)                          [32, 4096]           [32, 4096]           --                   --\n","========================================================================================================================\n","Total params: 57,003,840\n","Trainable params: 0\n","Non-trainable params: 57,003,840\n","Total mult-adds (G): 22.82\n","========================================================================================================================\n","Input size (MB): 19.79\n","Forward/backward pass size (MB): 128.07\n","Params size (MB): 228.02\n","Estimated Total Size (MB): 375.88\n","========================================================================================================================"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["# Extract features from the training data\n","train_features = []\n","train_labels = []\n","model = set_AlexNet()\n","model.eval()\n","\n","for X,y in train_dataloader:\n","  features = model(X)\n","  train_features.append(features)\n","  train_labels.append(y)\n","\n","\n","\n","\n","print(train_features.shape)\n","print(train_labels)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":398},"id":"sVqjGAiF4I8P","executionInfo":{"status":"error","timestamp":1678477472960,"user_tz":-210,"elapsed":1081719,"user":{"displayName":"yasin","userId":"00554470259111997069"}},"outputId":"ca410eb0-1f7d-43b9-9d2d-45a1d0ca2b48"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stderr","text":["Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-b39ef3c07e6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m   \u001b[0mtrain_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mtrain_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torchvision/models/alexnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavgpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/pooling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m         return F.max_pool2d(input, self.kernel_size, self.stride,\n\u001b[0m\u001b[1;32m    167\u001b[0m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mceil_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m                             return_indices=self.return_indices)\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/_jit_internal.py\u001b[0m in \u001b[0;36mfn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    483\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mif_true\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 485\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mif_false\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mif_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mif_false\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36m_max_pool2d\u001b[0;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[1;32m    780\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstride\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m         \u001b[0mstride\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 782\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mceil_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    783\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    784\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["import numpy as np\n","import os\n","from keras.preprocessing import image\n","from keras.applications import AlexNet\n","from keras.models import Model\n","from sklearn.svm import SVC\n","\n","# Load the pre-trained AlexNet model\n","alexnet = AlexNet(weights='imagenet')\n","\n","# Remove the last layer (the output layer) from the model\n","alexnet = Model(inputs=alexnet.inputs, outputs=alexnet.layers[-2].output)\n","\n","# Set up the paths to your training and test data directories\n","train_dir = 'path/to/your/training/data/directory'\n","test_dir = 'path/to/your/test/data/directory'\n","\n","# Set up the parameters for feature extraction and SVM classification\n","num_classes = 10  # number of classes in your dataset\n","batch_size = 64  # batch size for feature extraction\n","C = 1.0  # SVM regularization parameter\n","\n","# Extract features from the training data\n","train_features = []\n","train_labels = []\n","\n","for subdir in os.listdir(train_dir):\n","    label = int(subdir)  # the subdirectory name is the class label\n","    subpath = os.path.join(train_dir, subdir)\n","    for file in os.listdir(subpath):\n","        img_path = os.path.join(subpath, file)\n","        img = image.load_img(img_path, target_size=(227, 227))\n","        x = image.img_to_array(img)\n","        x = np.expand_dims(x, axis=0)\n","        x = preprocess_input(x)\n","        features = alexnet.predict(x)\n","        train_features.append(features)\n","        train_labels.append(label)\n","\n","train_features = np.array(train_features).reshape(-1, 4096)\n","train_labels = np.array(train_labels)\n","\n","# Train an SVM model\n","svm = SVC(kernel='linear', C=C)\n","svm.fit(train_features, train_labels)\n","\n","# Extract features from the test data and use the SVM model to predict the labels\n","test_features = []\n","test_filenames = []\n","\n","for file in os.listdir(test_dir):\n","    img_path = os.path.join(test_dir, file)\n","    img = image.load_img(img_path, target_size=(227, 227))\n","    x = image.img_to_array(img)\n","    x = np.expand_dims(x, axis=0)\n","    x = preprocess_input(x)\n","    features = alexnet.predict(x)\n","    test_features.append(features)\n","    test_filenames.append(file)\n","\n","test_features = np.array(test_features).reshape(-1, 4096)\n","test_labels = svm.predict(test_features)\n","\n","# Print the predicted labels for the test data\n","for i in range(len(test_filenames)):\n","    print(test_filenames[i], test_labels[i])"],"metadata":{"id":"UU2TJDVHx5YH"},"execution_count":null,"outputs":[]}]}