{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOfhZDCq6QRIHZMQqLToCVc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"l1P-S7y9hq4L"},"outputs":[],"source":["import time \n","import torch\n","import torch.nn as nn\n","import torchvision.datasets as datasets\n","import torch.optim as optim\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader\n","from torch.autograd import Variable\n","\n","!pip install torchsummaryX --quiet\n","from torchsummaryX import summary as summaryX\n","from torchsummary import summary\n","\n","from torch.utils.tensorboard import SummaryWriter\n","writer = SummaryWriter()"]},{"cell_type":"code","source":["class LeNet(nn.Module):\n","  def __init__(self):\n","    super(LeNet, self).__init__()\n","\n","    self.conv1 = nn.Conv2d(in_channels=1, out_channels=6,\n","                           kernel_size=5, stride=1, padding=0)\n","    self.conv2 = nn.Conv2d(in_channels=6, out_channels=16,\n","                           kernel_size=5, stride=1, padding=0)\n","    self.conv3 = nn.Conv2d(in_channels=16, out_channels=120,\n","                           kernel_size=5, stride=1, padding=0)\n","    self.linear1 = nn.Linear(in_features=120, out_features=84)\n","    self.linear2 = nn.Linear(in_features=84, out_features=10)\n","    self.tanh = nn.Tanh()\n","    self.avgpool = nn.AvgPool2d(kernel_size=2, stride=2)\n","\n","\n","  def forward(self,x):\n","    x = self.conv1(x)\n","    x = self.tanh(x)\n","    x = self.avgpool(x)\n","    x = self.conv2(x)\n","    x = self.tanh(x)\n","    x = self.avgpool(x)\n","    x = self.conv3(x)\n","    x = self.tanh(x)\n","\n","    x = x.reshape(x.shape[0], -1)\n","    x = self.linear1(x)\n","    x = self.tanh(x)\n","    x = self.linear2(x)\n","    return x\n","\n","model = LeNet()\n","model"],"metadata":{"id":"0AT7zPioi4mX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1675587192403,"user_tz":-210,"elapsed":538,"user":{"displayName":"yasin","userId":"00554470259111997069"}},"outputId":"4c7f6eeb-2c8b-4655-d00f-bab2d52f46ac"},"execution_count":44,"outputs":[{"output_type":"execute_result","data":{"text/plain":["LeNet(\n","  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n","  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n","  (conv3): Conv2d(16, 120, kernel_size=(5, 5), stride=(1, 1))\n","  (linear1): Linear(in_features=120, out_features=84, bias=True)\n","  (linear2): Linear(in_features=84, out_features=10, bias=True)\n","  (tanh): Tanh()\n","  (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",")"]},"metadata":{},"execution_count":44}]},{"cell_type":"code","source":["x = torch.randn(64,1,32,32)\n","output = model(x)\n","print(output.shape)\n","summary(model, (1,32,32))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QOC1ZNjIsavq","executionInfo":{"status":"ok","timestamp":1675587194575,"user_tz":-210,"elapsed":344,"user":{"displayName":"yasin","userId":"00554470259111997069"}},"outputId":"58aee58a-5ed1-48e2-d16f-c1c3d52e1ae3"},"execution_count":45,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([64, 10])\n","----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1            [-1, 6, 28, 28]             156\n","              Tanh-2            [-1, 6, 28, 28]               0\n","         AvgPool2d-3            [-1, 6, 14, 14]               0\n","            Conv2d-4           [-1, 16, 10, 10]           2,416\n","              Tanh-5           [-1, 16, 10, 10]               0\n","         AvgPool2d-6             [-1, 16, 5, 5]               0\n","            Conv2d-7            [-1, 120, 1, 1]          48,120\n","              Tanh-8            [-1, 120, 1, 1]               0\n","            Linear-9                   [-1, 84]          10,164\n","             Tanh-10                   [-1, 84]               0\n","           Linear-11                   [-1, 10]             850\n","================================================================\n","Total params: 61,706\n","Trainable params: 61,706\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.00\n","Forward/backward pass size (MB): 0.11\n","Params size (MB): 0.24\n","Estimated Total Size (MB): 0.35\n","----------------------------------------------------------------\n"]}]},{"cell_type":"markdown","source":["# Loading MNIST"],"metadata":{"id":"JYh6bYMcx8IG"}},{"cell_type":"markdown","source":["CUDA (Compute Unified Device Architecture) is a programming model and parallel computing platform developed by Nvidia. Using CUDA, one can maximize the utilization of Nvidia-provided GPUs, thereby improving the computation power and performing operations away faster by parallelizing the tasks. PyTorch provides a torch.cuda library to set up and run the CUDA operations. Before using the CUDA, we have to make sure whether CUDA is supported by our System.\n"],"metadata":{"id":"_iuD5Fxi0IkP"}},{"cell_type":"code","source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"],"metadata":{"id":"-QQEG3AUvzbN","executionInfo":{"status":"ok","timestamp":1675593951660,"user_tz":-210,"elapsed":1460,"user":{"displayName":"yasin","userId":"00554470259111997069"}}},"execution_count":54,"outputs":[]},{"cell_type":"code","source":["learning_rate = 0.01\n","num_epochs = 10\n","\n","train_dataset = datasets.MNIST(root='dataset/', train=True, transform=transforms.Compose([transforms.Pad(2), transforms.ToTensor()]), download=True)\n","valid_dataset = datasets.MNIST(root='dataset/', train=False, transform=transforms.Compose([transforms.Pad(2), transforms.ToTensor()]), download=True)\n","\n","train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n","valid_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=True)\n","dataset_sizes = {'train': len(train_dataset), 'test' : len(test_dataset)}\n","\n","model = LeNet().to(device)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)"],"metadata":{"id":"scT1hAdf0SDH","executionInfo":{"status":"ok","timestamp":1675598697653,"user_tz":-210,"elapsed":1271,"user":{"displayName":"yasin","userId":"00554470259111997069"}}},"execution_count":59,"outputs":[]},{"cell_type":"markdown","source":["* model.train() ---> tells your model that you are training the model. This helps inform layers such as Dropout and BatchNorm, which are designed to behave differently during training and evaluation. For instance, in training mode, BatchNorm updates a moving average on each new batch; whereas, for evaluation mode, these updates are frozen.\n","\n","* optimizer.zero_grad() ---> optimizer.zero_grad() clears x.grad for every parameter x in the optimizer. It’s important to call this before loss.backward(), otherwise you’ll accumulate the gradients from multiple passes.\n","\n","* loss.backward() ---> computes dloss/dx for every parameter x which has requires_grad=True. These are accumulated into x.grad for every parameter x.\n","      x.grad += dloss/dx\n","\n","* optimizer.step updates the value of x using the gradient x.grad.\n","      x += -lr * x.grad\n","\n","* torch.no_grad() ---> Disabling gradient calculation is useful for inference, when you are sure that you will not call Tensor.backward(). It will reduce memory consumption for computations that would otherwise have requires_grad=True. In this mode, the result of every computation will have requires_grad=False, even when the inputs have requires_grad=True."],"metadata":{"id":"m0xQv49NSo7b"}},{"cell_type":"code","source":["def train(train_loader, model,criterion, optimizer, device):\n","    '''\n","    Function for the training step of the training loop\n","    '''\n","    \n","    model.train()    \n","    running_loss = 0\n","\n","    for X, y_true in train_loader:\n","\n","        optimizer.zero_grad()           # Sets the gradients of all optimized torch.Tensor s to zero.\n","\n","        X = X.to(device)\n","        y_true = y_true.to(device)\n","\n","        # forward paas\n","        y_hat, _ = model(X)\n","        loss = criterion(y_hat, y_true)\n","        running_loss += loss.item()*X.size(0)        # X.size() ----> Returns the size of the self tensor\n","\n","        # Backward pass\n","        loss.backward()\n","        optimizer.step()\n","\n","    epoch_loss = running_loss / len(train_loader.dataset)\n","      \n","    return model, optimizer, epoch_loss\n","\n","def validate(valid_loader, model, criterion, device):\n","    '''\n","    Function for the validation step of the training loop\n","    '''\n","   \n","    model.eval()\n","    running_loss = 0\n","    \n","    for X, y_true in valid_loader:\n","    \n","        X = X.to(device)\n","        y_true = y_true.to(device)\n","\n","        # Forward pass and record loss\n","        y_hat, _ = model(X) \n","        loss = criterion(y_hat, y_true) \n","        running_loss += loss.item() * X.size(0)\n","\n","    epoch_loss = running_loss / len(valid_loader.dataset)\n","        \n","    return model, epoch_loss\n","\n","def training_loop(model, criterion, optimizer, train_loader, valid_loader,epochs, device, print_every=1):\n","    '''\n","    Function defining the entire training loop\n","    '''\n","\n","    # set objects for storing metrics\n","    best_loss = 10e10\n","    train_losses = []\n","    valid_losses = []\n","\n","\n","    # Train model\n","    for epoch in range(0,epochs):\n","\n","        # training\n","        model, optimizer, train_loss = train(train_loader, model,criterion, optimizer, device)\n","        train.losses.append(train_loss)\n","\n","        # validation\n","        with torch.no_grad():\n","            model, valid_loss = validate(valid_loader, model, criterion, device)\n","            valid_losses.append(valid_loss)\n","\n","        if epoch % print_every == (print_every -1):\n","\n","            train_acc = get_accuracy(model, train_loader, device=device)\n","            valid_acc = get_accuracy(model, valid_loader, device=device)\n","\n","            print(f'')\n","            \n","\n","\n","\n"],"metadata":{"id":"ltHZCfB-LgDa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"4_PgkjIARVDJ"},"execution_count":null,"outputs":[]}]}